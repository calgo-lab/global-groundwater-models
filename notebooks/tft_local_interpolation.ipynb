{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beed9cf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install \"pandas<2.0.0\"\n",
    "#!pip install \"pytorch-forecasting[mqf2]<1.0.0\"\n",
    "#!pip install numpy matplotlib pyarrow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79ae6c14",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pytorch_lightning as pl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6710858-e085-4ab5-9503-a92784022e38",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc5689e3",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e4d5cd1-a65f-477a-b41d-9e29b4af6a87",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "BASE_PATH = '/home/carl/projects/gwl_neu'\n",
    "\n",
    "DATA_PATH = os.path.join(BASE_PATH, 'data')\n",
    "MODEL_PATH = os.path.join(BASE_PATH, 'models')\n",
    "RESULT_PATH = os.path.join(BASE_PATH, 'results')\n",
    "\n",
    "LAG = 52  # weeks\n",
    "LEAD = 8  # weeks\n",
    "TRAIN_PERIOD = (pd.Timestamp(1990, 1, 1), pd.Timestamp(2012, 1, 1))\n",
    "TEST_PERIOD = (pd.Timestamp(2012, 1, 1), pd.Timestamp(2016, 1, 1))\n",
    "\n",
    "TIME_IDX = pd.date_range(TRAIN_PERIOD[0], TEST_PERIOD[1], freq='W-SUN', closed=None, name='time').to_frame().reset_index(drop=True)\n",
    "TIME_IDX.index.name = 'time_idx'\n",
    "TIME_IDX = TIME_IDX.reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12cbf322-1fd3-4460-abb4-2a66c8d96e7e",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a73000fd-544b-42c7-a8de-28fcd982078f",
   "metadata": {
    "tags": []
   },
   "source": [
    "### load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a71a6eab-9c03-4573-815b-d37818f1d077",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "static_df = pd.read_feather(os.path.join(DATA_PATH, 'static.feather'))\n",
    "df = pd.read_feather(os.path.join(DATA_PATH, 'temporal.feather'))\n",
    "df = df.merge(TIME_IDX, on='time', how='left')\n",
    "df = df.merge(static_df.drop(columns=['y', 'x']), on='proj_id', how='left')\n",
    "\n",
    "# encode day of the year as circular feature\n",
    "df['day_sin'] = np.sin(2*np.pi / 365. * df['time'].dt.dayofyear).astype(np.float32)\n",
    "df['day_cos'] = np.cos(2*np.pi / 365. * df['time'].dt.dayofyear).astype(np.float32)\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea1fce42-7cf0-4568-8eb1-375470e8e984",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "N_NEIGHBORS = 6\n",
    "MAX_DIST = 120.\n",
    "\n",
    "def merge_random_neighbor_wells(df, n_neighbors, max_dist, n_samplings=1):\n",
    "    _df = df[['proj_id', 'time_idx']].copy()\n",
    "    _df['diff'] = _df.groupby('proj_id')['time_idx'].apply(lambda s: s - pd.Series(s).shift(1))\n",
    "    _df['diff'] = ~(_df['diff'].fillna(1.) == 1.)\n",
    "    _df['cumsum'] = _df.groupby('proj_id')['diff'].cumsum()\n",
    "    periods = _df.groupby(['proj_id', 'cumsum'])['time_idx'].agg(['min', 'max'])\n",
    "    periods = periods.reset_index().merge(static_df[['proj_id', 'y', 'x']], how='left', on='proj_id').set_index(['proj_id', 'cumsum'])\n",
    "    \n",
    "    _dfs = []\n",
    "    for sampling in range(n_samplings):\n",
    "        matches = []\n",
    "        for idx, (min_ts, max_ts, y, x) in periods.iterrows():\n",
    "            matching_periods = periods[\n",
    "                (periods['min'] <= min_ts) & \n",
    "                (periods['max'] >= max_ts) & \n",
    "                (periods.index.get_level_values('proj_id') != idx[0]) & \n",
    "                (np.sqrt((periods['x'] - x)**2  + (periods['y'] - y)**2) < max_dist)\n",
    "            ]\n",
    "            sample_weights = -np.sqrt((matching_periods['x'] - x)**2  + (matching_periods['y'] - y)**2)\n",
    "            sample_weights -= sample_weights.min() - 0.001\n",
    "            sample = matching_periods.sample(n_neighbors, weights=sample_weights, replace=False)\n",
    "            record = {'proj_id': idx[0], 'cumsum': idx[1], 'min_ts': min_ts, 'max_ts': max_ts, 'sampling': sampling+1}\n",
    "            for i, proj_id in enumerate(sample.index.get_level_values('proj_id')):\n",
    "                record[f'neighbor_{i}'] = proj_id\n",
    "            matches.append(record)\n",
    "        matches_df = pd.DataFrame.from_records(matches)\n",
    "\n",
    "        dfs = []\n",
    "        for idx, row in matches_df.iterrows():\n",
    "            _df = pd.DataFrame({'time_idx': range(row['min_ts'], row['max_ts']+1)})\n",
    "            _df['proj_id'] = row['proj_id']\n",
    "            _df['sampling'] = row['sampling']\n",
    "            for i in range(N_NEIGHBORS):\n",
    "                _df[f'neighbor_{i}'] = row[f'neighbor_{i}']\n",
    "            dfs.append(_df)\n",
    "        expanded = pd.concat(dfs)\n",
    "\n",
    "        __df = df.merge(expanded, on=['proj_id', 'time_idx'], how='left')\n",
    "\n",
    "        for i in range(n_neighbors):\n",
    "            __df = __df.merge(\n",
    "                df[\n",
    "                    ['gwl', 'humidity', 'temperature', 'precipitation', 'lai', 'land_cover', 'rock_type', 'geochemical_rock_type',\n",
    "                     'cavity_type', 'permeability', 'elevation', 'gw_recharge', 'percolation', \n",
    "                     'lat', 'lon', 'time_idx', 'proj_id']\n",
    "                ], \n",
    "                left_on=['time_idx', f'neighbor_{i}'], \n",
    "                right_on=['time_idx', 'proj_id'], \n",
    "                how='left', \n",
    "                suffixes=('', f'_n{i}')\n",
    "            )\n",
    "        _dfs.append(__df)    \n",
    "    return pd.concat(_dfs, axis=0).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d68bac1c-cca2-42f6-84dc-8fb8001508a8",
   "metadata": {},
   "source": [
    "### Cross Validation\n",
    "\n",
    "spatio-temporal cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dae4c7dc-e2e5-4805-a8b8-b9d9a915c1e6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_df = df[df['time'].between(*TRAIN_PERIOD)]\n",
    "train_df = train_df[~train_df['proj_id'].isin(test_wells['proj_id'])].reset_index(drop=True)\n",
    "train_df = merge_random_neighbor_wells(train_df, N_NEIGHBORS, MAX_DIST, n_samplings=2)\n",
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65c1010f",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_samples = df[df['time'].between(*TEST_PERIOD)].groupby('proj_id').count()['time']\n",
    "test_samples = test_samples[test_samples == test_samples.max()]\n",
    "test_wells = static_df[static_df['proj_id']isin(test_samples.index)].groupby('hyraum', group_keys=False).apply(lambda x: x.sample(frac=0.05, random_state=42))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3d599f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_wells =  ['BB_28401185', 'BB_29400520' 'BB_31419861', 'BB_31464654', 'BB_31471808', 'BB_32392310', \n",
    "               'BB_33392320', 'BB_33442430', 'BB_33470881', 'BB_34426025', 'BB_34442481', 'BB_34522486', \n",
    "               'BB_36422930', 'BB_36441936', 'BB_36441951', 'BB_36441990', 'BB_38441747', 'BE_7214', \n",
    "               'BW_101-763-1', 'BW_103-112-9', 'BW_105-065-1', 'BW_105-116-3', 'BW_108-114-3', 'BW_111-509-7', \n",
    "               'BW_112-069-6', 'BW_113-115-8', 'BW_122-113-5', 'BW_126-762-3', 'BW_134-770-6', 'BW_137-113-3', \n",
    "               'BW_138-771-6', 'BW_146-114-6', 'BW_146-115-8', 'BW_149-020-7', 'BW_156-068-4', 'BW_159-066-7', \n",
    "               'BW_160-770-4', 'BW_161-771-0', 'BW_176-772-0', 'BW_177-770-1', 'BW_178-258-5', 'BW_2003-569-2', \n",
    "               'BW_227-020-3', 'BW_228-258-4', 'BW_263-259-5', 'BW_274-162-3', 'BW_5008-606-9', 'BY_11002', \n",
    "               'BY_13143', 'BY_16278', 'BY_17188', 'BY_2148', 'BY_3129', 'BY_6160', 'BY_9248', 'BY_9282', \n",
    "               'HE_10072', 'HE_11747', 'HE_12512', 'HE_12930', 'HE_16458', 'HE_6253', 'HE_6972', 'HE_8496', \n",
    "               'NI_100000489', 'NI_100000646', 'NI_200000876', 'NI_200000894', 'NI_40000501', 'NI_400060391', \n",
    "               'NI_400080061', 'NI_400081051', 'NI_500000058', 'NI_500000263', 'NI_500000367', 'NI_500000526', \n",
    "               'NI_500000594', 'NI_9610477', 'NI_9610849', 'NI_9610883', 'NI_9700010', 'NI_9700085', 'NI_9700178', \n",
    "               'NI_9700191', 'NI_9700192', 'NI_9700200', 'NI_9700203', 'NI_9700274', 'NI_9700291', 'NI_9850220', \n",
    "               'NI_9850831', 'NI_9852864', 'NW_100135020', 'NW_10203680', 'NW_110060090', 'NW_21180301', \n",
    "               'NW_40306021', 'NW_59130453', 'NW_60080280', 'NW_60090315', 'NW_60230113', 'NW_60240222', \n",
    "               'NW_60240325', 'NW_60240430', 'NW_70201018', 'NW_80000125', 'NW_80302695', 'NW_91133002', \n",
    "               'NW_91141709', 'NW_91168806', 'NW_91168909', 'RP_2375109100', 'RP_2379177700', 'RP_2393163500', \n",
    "               'SH_10L03003002', 'SH_10L51049015', 'SH_10L51120002', 'SH_10L54091003', 'SH_10L56031004', 'SH_10L57068003', \n",
    "               'SH_10L58026002', 'SH_10L58028005', 'SH_10L58123003', 'SH_10L59035004', 'SH_10L62020008', 'SN_45503444', \n",
    "               'SN_46410441', 'SN_46421125', 'SN_47440188', 'SN_47500596', 'SN_4840B5000', 'SN_48431031', 'SN_49486604', \n",
    "               'SN_50496167', 'SN_51410936', 'SN_51416002', 'ST_32360068', 'ST_33340002', 'ST_34320014', 'ST_34360055', \n",
    "               'ST_38385181', 'ST_41360080', 'ST_42320029', 'ST_42438270', 'ST_43360009', 'ST_43409272', 'ST_44330402', \n",
    "               'ST_44380030', 'TH_4531230790', 'TH_4729230702', 'TH_4731230724', 'TH_4734901150', 'TH_5034210608', \n",
    "               'TH_5227240535', 'TH_5429240534', 'TH_5430240547', 'TH_5633900114']\n",
    "test_wells = static_df[static_df['proj_id'].isin(test_wells)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcb20a84-ca26-47e7-bb28-99ade7775f87",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_df = df[df['time'].between(*TEST_PERIOD)]\n",
    "test_df = merge_random_neighbor_wells(test_df, N_NEIGHBORS, MAX_DIST, n_samplings=9)\n",
    "test_df = test_df[test_df['proj_id'].isin(test_wells['proj_id'])].reset_index(drop=True)\n",
    "test_df.to_feather(os.path.join(RESULT_PATH, 'predictions', 'tft_local_interpolation_test_set.feather'))\n",
    "test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0d4cd90",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.read_feather(os.path.join(RESULT_PATH, 'predictions', 'tft_local_interpolation_test_set.feather'))\n",
    "test_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "587f2146-6580-47ea-a3c2-c4ac9c9d48b4",
   "metadata": {},
   "source": [
    "### Time Series Data Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7703f77-5485-443c-97ac-037357c000da",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pytorch_forecasting import TimeSeriesDataSet\n",
    "\n",
    "STATIC_REALS = [\"elevation\", \"gw_recharge\", \"percolation\", \"lat\", \"lon\"]\n",
    "STATIC_CATEGORICALS = [\"land_cover\", \"rock_type\", \"geochemical_rock_type\", \"cavity_type\", \"permeability\"]\n",
    "TIME_VARYING_KNOWN_REALS = ['humidity', 'precipitation', 'temperature', 'lai', 'day_sin', 'day_cos']\n",
    "\n",
    "train_ds = TimeSeriesDataSet(\n",
    "    train_df,\n",
    "    group_ids=[\"proj_id\", \"sampling\"],\n",
    "    target=\"gwl\",\n",
    "    time_idx=\"time_idx\",\n",
    "    min_encoder_length=LAG,\n",
    "    max_encoder_length=LAG,\n",
    "    min_prediction_length=LEAD,\n",
    "    max_prediction_length=LEAD,\n",
    "    static_reals=[f'{var}_n{i}' for i in range(N_NEIGHBORS) for var in STATIC_REALS] + STATIC_REALS,\n",
    "    static_categoricals=['g_land_cover', 'g_rock_type', 'g_geochemical_rock_type', 'g_cavity_type', 'g_permeability'],\n",
    "    time_varying_unknown_reals=[f'{var}_n{i}' for i in range(N_NEIGHBORS) for var in ['humidity', 'precipitation', 'temperature', 'lai', 'gwl']],\n",
    "    time_varying_known_reals=TIME_VARYING_KNOWN_REALS,\n",
    "    add_target_scales=False,\n",
    "    allow_missing_timesteps=True,\n",
    "    variable_groups={\n",
    "        'g_land_cover': ['land_cover'] + [f'land_cover_n{i}' for i in range(N_NEIGHBORS)],\n",
    "        'g_rock_type': ['rock_type'] + [f'rock_type_n{i}' for i in range(N_NEIGHBORS)],\n",
    "        'g_geochemical_rock_type': ['geochemical_rock_type'] + [f'geochemical_rock_type_n{i}' for i in range(N_NEIGHBORS)],\n",
    "        'g_cavity_type': ['cavity_type'] + [f'cavity_type_n{i}' for i in range(N_NEIGHBORS)],\n",
    "        'g_permeability': ['permeability'] + [f'permeability_n{i}' for i in range(N_NEIGHBORS)],       \n",
    "    }\n",
    ")\n",
    "\n",
    "train_ds.save(os.path.join(RESULT_PATH, 'preprocessing', 'train_tft_local_interpolation.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64a30937-c37b-4065-be2f-3917f252fef5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pytorch_forecasting import TimeSeriesDataSet\n",
    "\n",
    "train_ds = TimeSeriesDataSet.load(os.path.join(RESULT_PATH, 'preprocessing', 'train_tft_local_interpolation.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e8eaafe-e984-4f04-8289-96629c032ccb",
   "metadata": {},
   "source": [
    "### Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42346725-27b5-4ca7-8464-5e54cb8866c2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_dataloader = train_ds.to_dataloader(train=True, batch_size=2048, num_workers=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fac6f06-3df2-457b-8cfd-ab631dfd2453",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcdcc172",
   "metadata": {},
   "source": [
    "train a new model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4081b9b5-08e4-4dfd-b415-05346e8de907",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pytorch_forecasting.models.temporal_fusion_transformer import TemporalFusionTransformer\n",
    "\n",
    "model = TemporalFusionTransformer.from_dataset(train_ds)\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=3,\n",
    "    accelerator='gpu', \n",
    "    devices=1,\n",
    "    enable_model_summary=True,\n",
    ")\n",
    "trainer.fit(\n",
    "    model,\n",
    "    train_dataloaders=train_dataloader,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d91e3944",
   "metadata": {},
   "source": [
    "or load an existing one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60a5a8dc-97c2-4617-9714-a91abb327b92",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pytorch_forecasting.models.temporal_fusion_transformer import TemporalFusionTransformer\n",
    "\n",
    "\n",
    "MODEL_NAME = 'tft_local_interpolation.ckpt'\n",
    "\n",
    "model = TemporalFusionTransformer.load_from_checkpoint(os.path.join(MODEL_PATH, MODEL_NAME))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50ba6c13-b9c6-4814-aaec-a82efbc9a32c",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02403f50-f03e-4e3c-8ca4-e539ed4ad08c",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### predict test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2e8a097-c569-42d3-b3a1-6b4bfe924b19",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_ds = TimeSeriesDataSet.from_dataset(train_ds, test_df)\n",
    "test_dataloader = test_ds.to_dataloader(train=False, batch_size=2048, num_workers=2)\n",
    "raw_predictions, index = model.predict(test_dataloader, mode=\"raw\", return_index=True, show_progress_bar=True)\n",
    "q_predictions = raw_predictions['prediction'].numpy()\n",
    "np.save(os.path.join(RESULT_PATH, 'predictions', 'tft_local_interpolation_raw_predictions.npy'), q_predictions)\n",
    "index.to_feather(os.path.join(RESULT_PATH, 'predictions', 'tft_local_interpolation_prediction_index.feather'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "486607ba-c470-4dbc-ba79-27621525930f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "q_predictions = np.load(os.path.join(RESULT_PATH, 'predictions', 'tft_local_interpolation_raw_predictions.npy'))\n",
    "index = pd.read_feather(os.path.join(RESULT_PATH, 'predictions', 'tft_local_interpolation_prediction_index.feather'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eeb5a2b-3e62-4919-b194-b36648649464",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from utils import predictions_to_df\n",
    "\n",
    "\n",
    "predictions_df = predictions_to_df(index, np.transpose(q_predictions, (2, 1, 0))[3], ['proj_id', 'sampling'], TIME_IDX, LEAD)\n",
    "predictions_df = predictions_df.groupby(axis=0, level=[0, 2, 3]).mean()\n",
    "predictions_df = predictions_df.reset_index().merge(test_df.loc[test_df['sampling'] == 1, ['proj_id', 'time', 'gwl']], on=['proj_id', 'time'], how='left').set_index(['proj_id', 'time', 'horizon'])\n",
    "for q_idx, q_name in [(0, '02'), (1, '10'), (2, '25'), (4, '75'), (5, '90'), (6, '98')]:\n",
    "    q_df = predictions_to_df(index, np.transpose(q_predictions, (2, 1, 0))[q_idx], ['proj_id', 'sampling'], TIME_IDX, LEAD)\n",
    "    q_df = q_df.groupby(axis=0, level=[0, 2, 3]).mean()\n",
    "    q_df.rename(columns={'forecast': f'forecast_q{q_name}'}, inplace=True)\n",
    "    predictions_df = predictions_df.merge(q_df, left_index=True, right_index=True)\n",
    "predictions_df.reset_index().to_feather(os.path.join(RESULT_PATH, 'predictions', 'tft_local_interpolation_predictions.feather'))\n",
    "predictions_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deff2983",
   "metadata": {},
   "source": [
    "or load predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c2f08f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_df = pd.read_feather(os.path.join(RESULT_PATH, 'predictions', 'tft_local_interpolation_predictions.feather')).set_index(['proj_id', 'time', 'horizon'])\n",
    "predictions_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80db8d1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import plot_predictions\n",
    "\n",
    "plot_predictions(predictions_df, 'BB_28401185', horizon='all')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fa5920c-8ce7-4d6c-ad6b-9d87530aff70",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "178faadf-00ec-4828-85f3-5868aa35c3fb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from utils import get_metrics\n",
    "\n",
    "metrics_df = get_metrics(predictions_df.dropna())\n",
    "metrics_df.reset_index().to_feather(os.path.join(RESULT_PATH, 'metrics', 'tft_local_interpolation_metrics.feather'))\n",
    "metrics_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e8081d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_df = pd.read_feather(os.path.join(RESULT_PATH, 'metrics', 'tft_local_interpolation_metrics.feather')).set_index(['proj_id', 'horizon'])\n",
    "metrics_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1d0481b",
   "metadata": {},
   "source": [
    "### Error Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fde55248-fc29-4fe5-ab3c-8d38842cbab0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "N_NEIGHBORS = 6\n",
    "\n",
    "def haversine(lon1, lat1, lon2, lat2):\n",
    "    lon1, lat1, lon2, lat2 = np.radians([lon1, lat1, lon2, lat2])\n",
    "    dlon = lon2 - lon1\n",
    "    dlat = lat2 - lat1\n",
    "    haver_formula = np.sin(dlat/2)**2 + np.cos(lat1) * np.cos(lat2) * np.sin(dlon/2)**2\n",
    "    r = 6371\n",
    "    dist = 2 * r * np.arcsin(np.sqrt(haver_formula))\n",
    "    return pd.Series(dist)\n",
    "\n",
    "_static_df = static_df.set_index('proj_id')\n",
    "\n",
    "test_df_stats = []\n",
    "for proj_id, group in test_df.groupby('proj_id'):\n",
    "    distances = []\n",
    "    for i in range(N_NEIGHBORS):\n",
    "        distances.append(haversine(group['lon'], group['lat'], group[f'lon_n{i}'], group[f'lat_n{i}']).values)\n",
    "    mean_dist = np.mean(np.concatenate(distances))\n",
    "    group_hyraums = _static_df.loc[group[[f'neighbor_{n}' for n in range(N_NEIGHBORS)]].values.flatten(), 'hyraum'].values\n",
    "    proj_hyraum = _static_df.loc[proj_id, 'hyraum']\n",
    "    hyraum_homogenity = np.sum(group_hyraums == proj_hyraum).sum() / len(group_hyraums)\n",
    "    test_df_stats.append({\n",
    "        'proj_id': proj_id,\n",
    "        'mean_neighbor_dist': mean_dist,\n",
    "        'hyraum_homogenity': hyraum_homogenity,\n",
    "    })\n",
    "test_df_stats = pd.DataFrame.from_records(test_df_stats)\n",
    "test_df_stats.to_feather(os.path.join(RESULT_PATH, 'predictions', 'tft_local_interpolation_test_df_stats.feather'))\n",
    "test_df_stats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3240659b-1d1f-4bd7-a989-4a2391cbf3f0",
   "metadata": {},
   "source": [
    "### Model Interpretation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2fb0bf9-c2b4-4eef-9db5-9c0e328ca4f0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "interpretation = model.interpret_output(raw_predictions, reduction=\"sum\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef153ab8-4cb9-4f9e-8475-daab27a49c0d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "variable_importance = {\n",
    "    'static_variables': dict(list(zip(model.static_variables, (interpretation['static_variables'].numpy()/np.sum(interpretation['static_variables'].numpy())).tolist()))),\n",
    "    'encoder_variables': dict(list(zip(model.encoder_variables, (interpretation['encoder_variables'].numpy()/np.sum(interpretation['encoder_variables'].numpy())).tolist()))),\n",
    "    'decoder_variables': dict(list(zip(model.decoder_variables, (interpretation['decoder_variables'].numpy()/np.sum(interpretation['decoder_variables'].numpy())).tolist())))\n",
    "        \n",
    "}\n",
    "with open(os.path.join(RESULTS_PATH, 'interpreation', 'tft_local_interpolation_variable_importance.json'), 'w') as f:\n",
    "    json.dump(variable_importance, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
